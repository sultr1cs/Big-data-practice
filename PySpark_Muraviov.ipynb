{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8UyPhos4-NC",
        "outputId": "17313b22-2b81-4d0e-e11a-3793de8c1dbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n",
            "================================================================================\n",
            "ЧАСТИНА 1: Основні операції з DataFrame\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Встановлення PySpark\n",
        "!pip install pyspark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DoubleType\n",
        "from pyspark.sql.functions import col, split, explode, lower, regexp_replace, count, sum, avg, max, min, desc, year, month, format_number\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# Створення SparkSession\n",
        "spark = SparkSession.builder.master('local').appName('Lab4').getOrCreate()\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ЧАСТИНА 1: Основні операції з DataFrame\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Завдання 1: Створення DataFrame з складною структурою\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Дані з вкладеними структурами\n",
        "data = [\n",
        "    ((\"Іван\", \"Петрович\", \"Коваль\"), \"001\", (\"Київ\", \"вул. Хрещатик, 1\"), 35000, [\"Python\", \"Spark\", \"SQL\"]),\n",
        "    ((\"Марія\", \"Олександрівна\", \"Петренко\"), \"002\", (\"Львів\", \"вул. Стрийська, 25\"), 42000, [\"Java\", \"Scala\"]),\n",
        "    ((\"Олександр\", \"\", \"Сидоренко\"), \"003\", (\"Одеса\", \"вул. Дерибасівська, 10\"), 38000, [\"Python\", \"ML\"]),\n",
        "    ((\"Анна\", \"Василівна\", \"Іваненко\"), \"004\", (\"Харків\", \"просп. Науки, 5\"), 45000, [\"Data Science\", \"Python\", \"R\"]),\n",
        "    ((\"Дмитро\", \"Іванович\", \"Мельник\"), \"005\", (\"Дніпро\", \"вул. Набережна, 15\"), 40000, [\"Big Data\", \"Spark\"])\n",
        "]\n",
        "\n",
        "# Визначення схеми з вкладеними структурами\n",
        "schema = StructType([\n",
        "    StructField('name', StructType([\n",
        "        StructField('firstname', StringType(), True),\n",
        "        StructField('middlename', StringType(), True),\n",
        "        StructField('lastname', StringType(), True)\n",
        "    ])),\n",
        "    StructField('employee_id', StringType(), True),\n",
        "    StructField('address', StructType([\n",
        "        StructField('city', StringType(), True),\n",
        "        StructField('street', StringType(), True)\n",
        "    ])),\n",
        "    StructField('salary', IntegerType(), True),\n",
        "    StructField('skills', StringType(), True)\n",
        "])\n",
        "\n",
        "df1 = spark.createDataFrame(data=data, schema=schema)\n",
        "\n",
        "print(\"\\nСхема DataFrame:\")\n",
        "df1.printSchema()\n",
        "\n",
        "print(\"\\nВміст DataFrame:\")\n",
        "df1.show(truncate=False)\n",
        "\n",
        "# Демонстрація роботи з collect()\n",
        "print(\"\\n--- Демонстрація функції collect() ---\")\n",
        "print(\"\\n1. Отримання всіх елементів:\")\n",
        "all_data = df1.collect()\n",
        "print(f\"Кількість рядків: {len(all_data)}\")\n",
        "\n",
        "print(\"\\n2. Перший рядок:\")\n",
        "first_row = df1.collect()[0]\n",
        "print(first_row)\n",
        "\n",
        "print(\"\\n3. Перша комірка (об'єкт name):\")\n",
        "first_cell = df1.collect()[0][0]\n",
        "print(first_cell)\n",
        "\n",
        "print(\"\\n4. Ім'я першого співробітника:\")\n",
        "first_name = df1.collect()[0][0][0]\n",
        "print(f\"Ім'я: {first_name}\")\n",
        "\n",
        "print(\"\\n5. Вибірка певних стовпців:\")\n",
        "df1.select(\"name.firstname\", \"name.lastname\", \"address.city\", \"salary\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5si293Q6LP2",
        "outputId": "b82ed831-488f-4f32-9598-fce13e600d22"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "Завдання 1: Створення DataFrame з складною структурою\n",
            "================================================================================\n",
            "\n",
            "Схема DataFrame:\n",
            "root\n",
            " |-- name: struct (nullable = true)\n",
            " |    |-- firstname: string (nullable = true)\n",
            " |    |-- middlename: string (nullable = true)\n",
            " |    |-- lastname: string (nullable = true)\n",
            " |-- employee_id: string (nullable = true)\n",
            " |-- address: struct (nullable = true)\n",
            " |    |-- city: string (nullable = true)\n",
            " |    |-- street: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            " |-- skills: string (nullable = true)\n",
            "\n",
            "\n",
            "Вміст DataFrame:\n",
            "+--------------------------------+-----------+-------------------------------+------+-------------------------+\n",
            "|name                            |employee_id|address                        |salary|skills                   |\n",
            "+--------------------------------+-----------+-------------------------------+------+-------------------------+\n",
            "|{Іван, Петрович, Коваль}        |001        |{Київ, вул. Хрещатик, 1}       |35000 |[Python, Spark, SQL]     |\n",
            "|{Марія, Олександрівна, Петренко}|002        |{Львів, вул. Стрийська, 25}    |42000 |[Java, Scala]            |\n",
            "|{Олександр, , Сидоренко}        |003        |{Одеса, вул. Дерибасівська, 10}|38000 |[Python, ML]             |\n",
            "|{Анна, Василівна, Іваненко}     |004        |{Харків, просп. Науки, 5}      |45000 |[Data Science, Python, R]|\n",
            "|{Дмитро, Іванович, Мельник}     |005        |{Дніпро, вул. Набережна, 15}   |40000 |[Big Data, Spark]        |\n",
            "+--------------------------------+-----------+-------------------------------+------+-------------------------+\n",
            "\n",
            "\n",
            "--- Демонстрація функції collect() ---\n",
            "\n",
            "1. Отримання всіх елементів:\n",
            "Кількість рядків: 5\n",
            "\n",
            "2. Перший рядок:\n",
            "Row(name=Row(firstname='Іван', middlename='Петрович', lastname='Коваль'), employee_id='001', address=Row(city='Київ', street='вул. Хрещатик, 1'), salary=35000, skills='[Python, Spark, SQL]')\n",
            "\n",
            "3. Перша комірка (об'єкт name):\n",
            "Row(firstname='Іван', middlename='Петрович', lastname='Коваль')\n",
            "\n",
            "4. Ім'я першого співробітника:\n",
            "Ім'я: Іван\n",
            "\n",
            "5. Вибірка певних стовпців:\n",
            "+---------+---------+------+------+\n",
            "|firstname| lastname|  city|salary|\n",
            "+---------+---------+------+------+\n",
            "|     Іван|   Коваль|  Київ| 35000|\n",
            "|    Марія| Петренко| Львів| 42000|\n",
            "|Олександр|Сидоренко| Одеса| 38000|\n",
            "|     Анна| Іваненко|Харків| 45000|\n",
            "|   Дмитро|  Мельник|Дніпро| 40000|\n",
            "+---------+---------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Завдання 2: Мінімальна температура для кожної станції\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Читання даних з файлу problem2_data set.txt\n",
        "df_temp = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", \"false\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .load(\"problem2_data set.txt\")\n",
        "\n",
        "# Перейменування колонок\n",
        "df_temp = df_temp.select(\n",
        "    col(\"_c0\").alias(\"stationId\"),\n",
        "    col(\"_c1\").alias(\"date\"),\n",
        "    col(\"_c2\").alias(\"readingType\"),\n",
        "    col(\"_c3\").alias(\"temperature\")\n",
        ")\n",
        "\n",
        "print(\"\\nПерші 10 рядків вхідних даних:\")\n",
        "df_temp.show(10)\n",
        "\n",
        "# Фільтрація тільки TMIN і пошук мінімуму для кожної станції\n",
        "result_temp = df_temp.filter(col(\"readingType\") == \"TMIN\") \\\n",
        "    .groupBy(\"stationId\") \\\n",
        "    .agg(min(\"temperature\").alias(\"min_temperature\"))\n",
        "\n",
        "print(\"\\nМінімальна температура для кожної станції:\")\n",
        "result_temp.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ascq7GPU6dAR",
        "outputId": "88a3e5e9-655b-483c-a2a4-17ed17e16c7b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "Завдання 2: Мінімальна температура для кожної станції\n",
            "================================================================================\n",
            "\n",
            "Перші 10 рядків вхідних даних:\n",
            "+-----------+--------+-----------+-----------+\n",
            "|  stationId|    date|readingType|temperature|\n",
            "+-----------+--------+-----------+-----------+\n",
            "|ITE00100554|18000101|       TMAX|        -75|\n",
            "|ITE00100554|18000101|       TMIN|       -148|\n",
            "|GM000010962|18000101|       PRCP|          0|\n",
            "|EZE00100082|18000101|       TMAX|        -86|\n",
            "|EZE00100082|18000101|       TMIN|       -135|\n",
            "|ITE00100554|18000102|       TMAX|        -60|\n",
            "|ITE00100554|18000102|       TMIN|       -125|\n",
            "|GM000010962|18000102|       PRCP|          0|\n",
            "|EZE00100082|18000102|       TMAX|        -44|\n",
            "|EZE00100082|18000102|       TMIN|       -130|\n",
            "+-----------+--------+-----------+-----------+\n",
            "only showing top 10 rows\n",
            "\n",
            "\n",
            "Мінімальна температура для кожної станції:\n",
            "+-----------+---------------+\n",
            "|  stationId|min_temperature|\n",
            "+-----------+---------------+\n",
            "|ITE00100554|           -148|\n",
            "|EZE00100082|           -135|\n",
            "+-----------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Завдання 3: Топ-10 найпопулярніших слів\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Читання даних з файлу problem3_data set.txt\n",
        "df_text = spark.read.text(\"problem3_data set.txt\")\n",
        "\n",
        "print(\"\\nКількість рядків у файлі:\", df_text.count())\n",
        "\n",
        "# Розбиття тексту на слова, приведення до нижнього регістру\n",
        "words_df = df_text.select(explode(split(lower(col(\"value\")), \"\\\\s+\")).alias(\"word\"))\n",
        "\n",
        "# Видалення порожніх слів\n",
        "words_df = words_df.filter(col(\"word\") != \"\")\n",
        "\n",
        "word_counts = words_df.groupBy(\"word\") \\\n",
        "    .count() \\\n",
        "    .orderBy(desc(\"count\")) \\\n",
        "    .limit(10)\n",
        "\n",
        "print(\"\\nТоп-10 найпопулярніших слів:\")\n",
        "word_counts.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkImWY1N6hg_",
        "outputId": "6edb14f6-4c39-41b9-a00e-67ba9b19b4bd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "Завдання 3: Топ-10 найпопулярніших слів\n",
            "================================================================================\n",
            "\n",
            "Кількість рядків у файлі: 611\n",
            "\n",
            "Топ-10 найпопулярніших слів:\n",
            "+---------+-----+\n",
            "|word     |count|\n",
            "+---------+-----+\n",
            "|data     |361  |\n",
            "|big      |285  |\n",
            "|in       |171  |\n",
            "|training |114  |\n",
            "|course   |105  |\n",
            "|hadoop   |100  |\n",
            "|online   |58   |\n",
            "|courses  |53   |\n",
            "|spark    |42   |\n",
            "|bangalore|40   |\n",
            "+---------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Завдання 4: Топ-10 клієнтів з максимальними витратами\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Читання даних з файлу problem4_data set.txt\n",
        "df_customers = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", \"false\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .load(\"problem4_data set.txt\")\n",
        "\n",
        "# Перейменування колонок\n",
        "df_customers = df_customers.select(\n",
        "    col(\"_c0\").alias(\"CustomerId\"),\n",
        "    col(\"_c1\").alias(\"ProductId\"),\n",
        "    col(\"_c2\").alias(\"Amount\")\n",
        ")\n",
        "\n",
        "print(\"\\nПерші 10 рядків вхідних даних:\")\n",
        "df_customers.show(10)\n",
        "\n",
        "# Групування за клієнтами та підрахунок загальної суми витрат\n",
        "top_customers = df_customers.groupBy(\"CustomerId\") \\\n",
        "    .agg(sum(\"Amount\").alias(\"TotalSpent\")) \\\n",
        "    .orderBy(desc(\"TotalSpent\")) \\\n",
        "    .limit(10)\n",
        "\n",
        "print(\"\\nТоп-10 клієнтів за витратами:\")\n",
        "top_customers.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmIzRZq16iFn",
        "outputId": "f07100ed-d087-4d75-ad1f-fae30c122ba4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "Завдання 4: Топ-10 клієнтів з максимальними витратами\n",
            "================================================================================\n",
            "\n",
            "Перші 10 рядків вхідних даних:\n",
            "+----------+---------+------+\n",
            "|CustomerId|ProductId|Amount|\n",
            "+----------+---------+------+\n",
            "|        44|     8602| 37.19|\n",
            "|        35|     5368| 65.89|\n",
            "|         2|     3391| 40.64|\n",
            "|        47|     6694| 14.98|\n",
            "|        29|      680| 13.08|\n",
            "|        91|     8900| 24.59|\n",
            "|        70|     3959| 68.68|\n",
            "|        85|     1733| 28.53|\n",
            "|        53|     9900| 83.55|\n",
            "|        14|     1505|  4.32|\n",
            "+----------+---------+------+\n",
            "only showing top 10 rows\n",
            "\n",
            "\n",
            "Топ-10 клієнтів за витратами:\n",
            "+----------+-----------------+\n",
            "|CustomerId|       TotalSpent|\n",
            "+----------+-----------------+\n",
            "|        68|6375.449999999997|\n",
            "|        73|6206.199999999999|\n",
            "|        39|6193.109999999999|\n",
            "|        54|6065.389999999999|\n",
            "|        71|5995.660000000003|\n",
            "|         2|          5994.59|\n",
            "|        97|5977.189999999995|\n",
            "|        46|5963.109999999999|\n",
            "|        42|5696.840000000003|\n",
            "|        59|          5642.89|\n",
            "+----------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Завдання 5: Аналіз рейтингів фільмів\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Створення тестових даних (або завантажте problem5_data set.txt якщо є)\n",
        "ratings_data = [\n",
        "    (1, 101, 5, 1234567890),\n",
        "    (1, 102, 4, 1234567891),\n",
        "    (2, 101, 5, 1234567892),\n",
        "    (2, 103, 3, 1234567893),\n",
        "    (3, 104, 2, 1234567894),\n",
        "    (3, 105, 1, 1234567895),\n",
        "    (4, 106, 5, 1234567896),\n",
        "    (4, 107, 4, 1234567897),\n",
        "    (5, 108, 3, 1234567898),\n",
        "    (5, 109, 5, 1234567899),\n",
        "    (1, 110, 5, 1234567900),\n",
        "    (2, 111, 4, 1234567901),\n",
        "    (3, 112, 3, 1234567902),\n",
        "    (4, 113, 2, 1234567903),\n",
        "    (5, 114, 1, 1234567904),\n",
        "]\n",
        "\n",
        "ratings_columns = [\"userid\", \"movieid\", \"rating\", \"timestamp\"]\n",
        "df_ratings = spark.createDataFrame(ratings_data, ratings_columns)\n",
        "\n",
        "print(\"\\nВхідні дані:\")\n",
        "df_ratings.show(10)\n",
        "\n",
        "# Підрахунок кількості оцінок для кожної зірки\n",
        "rating_counts = df_ratings.groupBy(\"rating\") \\\n",
        "    .count() \\\n",
        "    .orderBy(desc(\"rating\"))\n",
        "\n",
        "print(\"\\nСтатистика рейтингів:\")\n",
        "rating_counts.show()\n",
        "\n",
        "# Відповіді на питання\n",
        "for r in range(5, 0, -1):\n",
        "    count_val = df_ratings.filter(col(\"rating\") == r).count()\n",
        "    print(f\"Фільми оцінювались {r} зірками: {count_val} разів\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QC_jsKJN6lhQ",
        "outputId": "96457787-eae9-4b8c-8de9-d50d74efb10b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "Завдання 5: Аналіз рейтингів фільмів\n",
            "================================================================================\n",
            "\n",
            "Вхідні дані:\n",
            "+------+-------+------+----------+\n",
            "|userid|movieid|rating| timestamp|\n",
            "+------+-------+------+----------+\n",
            "|     1|    101|     5|1234567890|\n",
            "|     1|    102|     4|1234567891|\n",
            "|     2|    101|     5|1234567892|\n",
            "|     2|    103|     3|1234567893|\n",
            "|     3|    104|     2|1234567894|\n",
            "|     3|    105|     1|1234567895|\n",
            "|     4|    106|     5|1234567896|\n",
            "|     4|    107|     4|1234567897|\n",
            "|     5|    108|     3|1234567898|\n",
            "|     5|    109|     5|1234567899|\n",
            "+------+-------+------+----------+\n",
            "only showing top 10 rows\n",
            "\n",
            "\n",
            "Статистика рейтингів:\n",
            "+------+-----+\n",
            "|rating|count|\n",
            "+------+-----+\n",
            "|     5|    5|\n",
            "|     4|    3|\n",
            "|     3|    3|\n",
            "|     2|    2|\n",
            "|     1|    2|\n",
            "+------+-----+\n",
            "\n",
            "Фільми оцінювались 5 зірками: 5 разів\n",
            "Фільми оцінювались 4 зірками: 3 разів\n",
            "Фільми оцінювались 3 зірками: 3 разів\n",
            "Фільми оцінювались 2 зірками: 2 разів\n",
            "Фільми оцінювались 1 зірками: 2 разів\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ЧАСТИНА 2: Аналіз даних фондового ринку (Yahoo Finance)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Створення тестових даних для демонстрації\n",
        "stock_data = [\n",
        "    (\"2010-01-04\", 30.57, 30.74, 30.21, 30.57, 123432400, 27.73),\n",
        "    (\"2010-01-05\", 30.66, 30.80, 30.46, 30.63, 150476200, 27.78),\n",
        "    (\"2010-01-06\", 30.63, 30.75, 30.11, 30.14, 138040000, 27.33),\n",
        "    (\"2011-01-03\", 45.52, 46.23, 45.12, 46.08, 111268600, 41.78),\n",
        "    (\"2012-01-03\", 58.75, 59.39, 57.87, 58.95, 140129500, 53.47),\n",
        "    (\"2015-01-02\", 111.39, 111.44, 107.35, 109.33, 53204600, 103.22),\n",
        "    (\"2016-01-04\", 102.61, 105.37, 102.00, 105.35, 67649400, 99.52),\n",
        "    (\"2018-01-02\", 170.16, 172.30, 169.26, 172.26, 118071600, 168.19),\n",
        "    (\"2020-01-02\", 296.24, 300.60, 295.19, 300.35, 135647200, 297.43),\n",
        "    (\"2020-03-23\", 224.37, 228.47, 214.88, 224.37, 178876600, 222.16),\n",
        "]\n",
        "\n",
        "stock_columns = [\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Adj Close\"]\n",
        "df_stock = spark.createDataFrame(stock_data, stock_columns)\n",
        "\n",
        "print(\"\\nДані завантажено успішно!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qN0Kjri36rAW",
        "outputId": "7e0e2165-e039-4c26-b97f-96010c7cc9bf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "ЧАСТИНА 2: Аналіз даних фондового ринку (Yahoo Finance)\n",
            "================================================================================\n",
            "\n",
            "Дані завантажено успішно!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Завдання 1: Схема DataFrame\n",
        "print(\"\\n1. Схема DataFrame:\")\n",
        "df_stock.printSchema()\n",
        "\n",
        "# Завдання 2: Перші 10 рядків\n",
        "print(\"\\n2. Перші 10 рядків з набору даних:\")\n",
        "df_stock.show(10)\n",
        "\n",
        "# Завдання 3: HV Ratio\n",
        "print(\"\\n3. DataFrame зі стовпцем HV Ratio:\")\n",
        "df_stock_hv = df_stock.withColumn(\"HV_Ratio\", col(\"High\") / col(\"Volume\"))\n",
        "df_stock_hv.select(\"Date\", \"High\", \"Volume\", \"HV_Ratio\").show(10)\n",
        "\n",
        "# Завдання 4: День з піковою ціною\n",
        "print(\"\\n4. День з піковою ціною Close:\")\n",
        "peak_day = df_stock.orderBy(desc(\"Close\")).limit(1)\n",
        "peak_day.select(\"Date\", \"Close\").show()\n",
        "\n",
        "# Завдання 5: Середнє значення Close\n",
        "print(\"\\n5. Середнє значення стовпця Close:\")\n",
        "avg_close = df_stock.agg(avg(\"Close\").alias(\"Average_Close\"))\n",
        "avg_close.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0U78cR7_5rY",
        "outputId": "75bd6b1e-a08f-49ef-9b11-d50e30c10912"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1. Схема DataFrame:\n",
            "root\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Open: double (nullable = true)\n",
            " |-- High: double (nullable = true)\n",
            " |-- Low: double (nullable = true)\n",
            " |-- Close: double (nullable = true)\n",
            " |-- Volume: long (nullable = true)\n",
            " |-- Adj Close: double (nullable = true)\n",
            "\n",
            "\n",
            "2. Перші 10 рядків з набору даних:\n",
            "+----------+------+------+------+------+---------+---------+\n",
            "|      Date|  Open|  High|   Low| Close|   Volume|Adj Close|\n",
            "+----------+------+------+------+------+---------+---------+\n",
            "|2010-01-04| 30.57| 30.74| 30.21| 30.57|123432400|    27.73|\n",
            "|2010-01-05| 30.66|  30.8| 30.46| 30.63|150476200|    27.78|\n",
            "|2010-01-06| 30.63| 30.75| 30.11| 30.14|138040000|    27.33|\n",
            "|2011-01-03| 45.52| 46.23| 45.12| 46.08|111268600|    41.78|\n",
            "|2012-01-03| 58.75| 59.39| 57.87| 58.95|140129500|    53.47|\n",
            "|2015-01-02|111.39|111.44|107.35|109.33| 53204600|   103.22|\n",
            "|2016-01-04|102.61|105.37| 102.0|105.35| 67649400|    99.52|\n",
            "|2018-01-02|170.16| 172.3|169.26|172.26|118071600|   168.19|\n",
            "|2020-01-02|296.24| 300.6|295.19|300.35|135647200|   297.43|\n",
            "|2020-03-23|224.37|228.47|214.88|224.37|178876600|   222.16|\n",
            "+----------+------+------+------+------+---------+---------+\n",
            "\n",
            "\n",
            "3. DataFrame зі стовпцем HV Ratio:\n",
            "+----------+------+---------+--------------------+\n",
            "|      Date|  High|   Volume|            HV_Ratio|\n",
            "+----------+------+---------+--------------------+\n",
            "|2010-01-04| 30.74|123432400|2.490432009747845...|\n",
            "|2010-01-05|  30.8|150476200|2.046835313491436E-7|\n",
            "|2010-01-06| 30.75|138040000|2.227615184004636...|\n",
            "|2011-01-03| 46.23|111268600|4.154810970929803...|\n",
            "|2012-01-03| 59.39|140129500|4.238222501329127...|\n",
            "|2015-01-02|111.44| 53204600|2.094555733902707...|\n",
            "|2016-01-04|105.37| 67649400|1.557589572117417...|\n",
            "|2018-01-02| 172.3|118071600|1.459284027657794...|\n",
            "|2020-01-02| 300.6|135647200|2.216042793363962E-6|\n",
            "|2020-03-23|228.47|178876600|1.277249232152221E-6|\n",
            "+----------+------+---------+--------------------+\n",
            "\n",
            "\n",
            "4. День з піковою ціною Close:\n",
            "+----------+------+\n",
            "|      Date| Close|\n",
            "+----------+------+\n",
            "|2020-01-02|300.35|\n",
            "+----------+------+\n",
            "\n",
            "\n",
            "5. Середнє значення стовпця Close:\n",
            "+-------------+\n",
            "|Average_Close|\n",
            "+-------------+\n",
            "|      110.803|\n",
            "+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Завдання 6: Максимальне та мінімальне значення Volume\n",
        "print(\"\\n6. Максимальне та мінімальне значення Volume:\")\n",
        "volume_stats = df_stock.agg(\n",
        "    max(\"Volume\").alias(\"Max_Volume\"),\n",
        "    min(\"Volume\").alias(\"Min_Volume\")\n",
        ")\n",
        "volume_stats.show()\n",
        "\n",
        "# Завдання 7: Кількість днів з Close < 60\n",
        "print(\"\\n7. Кількість днів, коли Close < 60:\")\n",
        "days_below_60 = df_stock.filter(col(\"Close\") < 60).count()\n",
        "print(f\"Днів з Close < 60: {days_below_60}\")\n",
        "\n",
        "# Завдання 8: Відсоток часу, коли High > 80\n",
        "print(\"\\n8. Відсоток часу, коли High > 80:\")\n",
        "total_days = df_stock.count()\n",
        "days_high_above_80 = df_stock.filter(col(\"High\") > 80).count()\n",
        "percentage = (days_high_above_80 / total_days) * 100 if total_days > 0 else 0\n",
        "print(f\"Відсоток: {percentage:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ul9NHSBiAAWt",
        "outputId": "62f1a044-cf6a-445e-95a2-358f592c7b74"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "6. Максимальне та мінімальне значення Volume:\n",
            "+----------+----------+\n",
            "|Max_Volume|Min_Volume|\n",
            "+----------+----------+\n",
            "| 178876600|  53204600|\n",
            "+----------+----------+\n",
            "\n",
            "\n",
            "7. Кількість днів, коли Close < 60:\n",
            "Днів з Close < 60: 5\n",
            "\n",
            "8. Відсоток часу, коли High > 80:\n",
            "Відсоток: 50.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Завдання 9: Щорічний максимум High\n",
        "print(\"\\n9. Щорічний максимум High:\")\n",
        "# Конвертуємо стовпець Date у тип Date\n",
        "df_stock_date = df_stock.withColumn(\"Date\", col(\"Date\").cast(\"date\"))\n",
        "yearly_max = df_stock_date.withColumn(\"Year\", year(\"Date\")) \\\n",
        "    .groupBy(\"Year\") \\\n",
        "    .agg(max(\"High\").alias(\"Max_High\")) \\\n",
        "    .orderBy(\"Year\")\n",
        "yearly_max.show()\n",
        "\n",
        "# Завдання 10: Середнє Close для кожного місяця\n",
        "print(\"\\n10. Середнє Close для кожного календарного місяця:\")\n",
        "monthly_avg = df_stock_date.withColumn(\"Month\", month(\"Date\")) \\\n",
        "    .groupBy(\"Month\") \\\n",
        "    .agg(avg(\"Close\").alias(\"Avg_Close\")) \\\n",
        "    .orderBy(\"Month\")\n",
        "monthly_avg.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DU7syuPhAFxp",
        "outputId": "4cc345a8-346f-4148-ae01-f7e5259b172a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "9. Щорічний максимум High:\n",
            "+----+--------+\n",
            "|Year|Max_High|\n",
            "+----+--------+\n",
            "|2010|    30.8|\n",
            "|2011|   46.23|\n",
            "|2012|   59.39|\n",
            "|2015|  111.44|\n",
            "|2016|  105.37|\n",
            "|2018|   172.3|\n",
            "|2020|   300.6|\n",
            "+----+--------+\n",
            "\n",
            "\n",
            "10. Середнє Close для кожного календарного місяця:\n",
            "+-----+-----------------+\n",
            "|Month|        Avg_Close|\n",
            "+-----+-----------------+\n",
            "|    1|98.18444444444444|\n",
            "|    3|           224.37|\n",
            "+-----+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ВИСНОВКИ\")\n",
        "print(\"=\"*80)\n",
        "print(\"\"\"\n",
        "У ході виконання лабораторної роботи було:\n",
        "\n",
        "1. Освоєно основні операції створення DataFrame в PySpark різними способами:\n",
        "   - Ручне створення з складними вкладеними структурами\n",
        "   - Завантаження з файлів\n",
        "   - Перетворення з RDD\n",
        "\n",
        "2. Продемонстровано роботу з функцією collect() для отримання даних з DataFrame:\n",
        "   - Отримання всіх елементів\n",
        "   - Доступ до окремих рядків та комірок\n",
        "   - Робота зі складними структурами даних\n",
        "\n",
        "3. Виконано практичні завдання з аналізу даних:\n",
        "   - Агрегування даних за групами (groupBy)\n",
        "   - Фільтрація та сортування даних\n",
        "   - Робота з текстовими даними та підрахунок частоти слів\n",
        "   - Обчислення статистичних показників\n",
        "\n",
        "4. Проведено аналіз фінансових даних фондового ринку:\n",
        "   - Створення нових обчислюваних стовпців\n",
        "   - Знаходження максимальних, мінімальних та середніх значень\n",
        "   - Робота з датами та часовими рядами\n",
        "   - Групування даних за роками та місяцями\n",
        "\n",
        "PySpark DataFrame надає потужний та зручний інтерфейс для роботи з великими\n",
        "обсягами даних, підтримує SQL-подібний синтаксис та автоматично оптимізує\n",
        "виконання запитів завдяки вбудованому оптимізатору Catalyst.\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XW3e0C7bAGUf",
        "outputId": "f19ad3bf-abf0-41a1-eafa-ffa420ed1e6e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "ВИСНОВКИ\n",
            "================================================================================\n",
            "\n",
            "У ході виконання лабораторної роботи було:\n",
            "\n",
            "1. Освоєно основні операції створення DataFrame в PySpark різними способами:\n",
            "   - Ручне створення з складними вкладеними структурами\n",
            "   - Завантаження з файлів\n",
            "   - Перетворення з RDD\n",
            "\n",
            "2. Продемонстровано роботу з функцією collect() для отримання даних з DataFrame:\n",
            "   - Отримання всіх елементів\n",
            "   - Доступ до окремих рядків та комірок\n",
            "   - Робота зі складними структурами даних\n",
            "\n",
            "3. Виконано практичні завдання з аналізу даних:\n",
            "   - Агрегування даних за групами (groupBy)\n",
            "   - Фільтрація та сортування даних\n",
            "   - Робота з текстовими даними та підрахунок частоти слів\n",
            "   - Обчислення статистичних показників\n",
            "\n",
            "4. Проведено аналіз фінансових даних фондового ринку:\n",
            "   - Створення нових обчислюваних стовпців\n",
            "   - Знаходження максимальних, мінімальних та середніх значень\n",
            "   - Робота з датами та часовими рядами\n",
            "   - Групування даних за роками та місяцями\n",
            "\n",
            "PySpark DataFrame надає потужний та зручний інтерфейс для роботи з великими\n",
            "обсягами даних, підтримує SQL-подібний синтаксис та автоматично оптимізує\n",
            "виконання запитів завдяки вбудованому оптимізатору Catalyst.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "75WX6uiFAKjl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}